# Modular Endpoint Crawler

A flexible, highâ€‘coverage static analysis tool to discover all HTTP endpoints in a legacy Java/Spring Boot codebase.

## Features

- **AST Annotation Extraction** (`@RequestMapping`, `@GetMapping`, etc.)
- **JAXâ€‘RS** `@Path` scanning
- **Programmatic Registration** hooks (`registerMapping`, `addServlet`)
- **Spring WebFlux Functional Routes** via `RequestPredicates`
- **Static Resource & View Handlers** (`addResourceHandler`, `addViewController`)
- **Security & CORS Config** (`antMatchers`)
- **Error & Fallback Mappings** (`ErrorController`, `@ControllerAdvice`)
- **XML Config Parsing** (`web.xml`, Struts `.template`)
- **JSP View Discovery** (when mapped)
- **Constant Resolution** for `static final String` paths
- **Parameter Inference** for `{param}` segments
- **Robust Encoding Support**: automatic fallback across UTFâ€‘8, UTFâ€‘16, Latinâ€‘1, CP1252, then ignore errors
- **Dynamic Detector Registration**: drop new detector modules into the `detectors/` directory; register each with a tag and function
- **Multiâ€‘Input Scanning**: accept multiple directories, files, or ZIP/WAR archives in one run
- **Modular Codebase**: `helpers.py`, `detectors/`, `crawler.py`
- **Rich CLI Output**: colorâ€‘coded, rounded tables via [Rich](https://github.com/Textualize/rich)
- **Multiple Output Formats**: CSV, JSON, Markdown, Postman Collection

## Installation

```bash
pip install javalang rich
```

Clone the repo:

```bash
git clone <repo-url>
cd endpoint_crawler_modular
```

## Directory Structure

```
endpoint_crawler_modular/
â”œâ”€â”€ crawler.py         # Main entrypoint
â”œâ”€â”€ helpers.py         # Utility functions (file I/O, parsing, encoding)
â””â”€â”€ detectors/         # One module per detection strategy
    â”œâ”€â”€ ast_detector.py
    â”œâ”€â”€ jaxrs_detector.py
    â”œâ”€â”€ programmatic_detector.py
    â”œâ”€â”€ webflux_detector.py
    â”œâ”€â”€ static_detector.py
    â”œâ”€â”€ security_detector.py
    â”œâ”€â”€ error_detector.py
    â”œâ”€â”€ xml_detector.py
    â””â”€â”€ jsp_detector.py
```

## Usage

```bash
python crawler.py \
  -i /path/to/dir file.java archive.zip myapp.war \
  -o endpoints.csv \
  -f csv
```

| Option        | Description                                                     | Default |
|---------------|-----------------------------------------------------------------|---------|
| `-i, --inputs`| One or more directories, individual files, or ZIP/WAR archives   | (none)  |
| `-o, --output`| Path where results will be written (CSV, JSON, Markdown, Postman)| (none)  |
| `-f, --format`| Output format (`csv`, `json`, `markdown`, `postman`)            | `csv`   |

## Output

- **CSV/JSON/Markdown**: columns/fields:
  - `endpoint` â€” the URL path
  - `confidence` â€” % score (100 = at least two independent detections)
  - `reason` â€” which detectors (tags) agreed
  - `params` â€” commaâ€‘separated `{â€¦}` variables
  - `locations` â€” `file:line` entries of each detection

- **Postman Collection**:
  - Exports a v2.1.0 collection with GET requests (`{{baseUrl}}` placeholder)
  - Includes `locations` in the request description

## Extending with New Detectors

1. Create a new file in `detectors/`, e.g. `my_detector.py`.
2. Export a function `my_detector(origin: str, text: str) -> List[dict]`:
   ```python
   def my_detector(origin, text):
       # analyze the provided `text`, return list of dicts
       # each dict must have keys: endpoint, line, method, controller, source, file
   ```
3. In `crawler.py`, add an entry to the `DETECTORS` list:
   ```python
   DETECTORS.append(('MYTAG', my_detector))
   ```
4. Re-run the crawler.

## Notes

- **Multiâ€‘Input Support**: scan multiple directories, files, and archives in one pass.
- **Inâ€‘Memory Scanning**: detectors operate on provided text for both disk files and archive entries.
- **Dynamic Registration**: detectors are tagged for provenance; drop new modules into `detectors/` and register them.
- **Encoding Robustness**: files are decoded with a fallback chain (UTFâ€‘8 â†’ UTFâ€‘16 â†’ Latinâ€‘1 â†’ CP1252 â†’ ignore errors).
- **Performance**: removing predicate lambdas means all detectors run on all inputsâ€”tune as needed for large codebases.
- Results are grouped by endpoint, deduplicated, and scored by multiâ€‘source provenance.

---

Happy crawling! ðŸš€
